{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "파라미터 값들을 조정해서 오차를 줄이는 학습과정을 생각해보자. 간단한 신경망에서는 파라미터를 조금씩 바꾸면서 정방향(forward)로 오차의 변화를 계산하고 파라미터 값을 조정하면 된다. 하지만 딥러닝에서는 매우 많은 파라미터가 있어서 이 과정에서 매우 많은 계산이 필요하다. 오차역전파(Error backpropagation)은 이런 상황에서 오차를 최소화하는 파라미터를 찾는 효과적인 방법이다. 이런 이유로 Backpropagation은 딥러닝에서 핵심적인 역할을 한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "출력층에 $n$개의 노드가 있다면 오차(MSE: mean squared error)는 다음과 같다.\n",
    "$$ E = \\frac{1}{2} \\sum_i (y_i - o_i)^2.$$\n",
    "$y_i$는 실제값, $o_i$는 예측 혹은 관측값이다. 우리가 찾아야하는 것은 파라미터를 바꾸었을 때 에너지의 변화이다.\n",
    "$$ \\frac{\\partial E}{\\partial w_{ij}}.$$ \n",
    "간단한 체인룰을 써서 다음처럼 기술해보자.\n",
    "$$ \\frac{\\partial E}{\\partial w_{ij}}=\\frac{\\partial E}{\\partial o_{j}} \\frac{\\partial o_j}{\\partial net_{j}} \\frac{\\partial net_j}{\\partial w_{ij}}.$$ \n",
    "여기에서 $net_j$는 노드에 들어오는 입력의 합을, $o_j$는 노드의 출력값을 뜻한다. 즉 $net_j$는 $\\sum w x$, $o_j$는 $f(net_j)$를 뜻한다. \n",
    " \n",
    "위의 변화율 식에서 3개의 항들을 각각 살펴보자.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. $\\frac{\\partial E}{\\partial o_j}$: 노드의 출력값 변화에 따른 오차의 변화율\n",
    "$$\\frac{\\partial E}{\\partial o_j} = \\frac{\\partial}{\\partial o_j} \\sum_k \\frac{1}{2} (y_k-o_k)^2 = o_j -y_j.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. $\\frac{\\partial o_j}{\\partial net_j}$: 입력합의 변화에 따른 노드의 출력값 변화율\n",
    "activation 함수를 sigmoid 함수로 가정하면,\n",
    "$$\\frac{\\partial o_j}{\\partial net_j} = \\frac{\\partial f(net_j)}{\\partial net_j}=f(net_j) [1- f(net_j)].$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. $\\frac{\\partial net_j}{\\partial w_{ij}}$: 가중치의 변화에 따른 입력합의 변화율 \n",
    "$$\\frac{\\partial net_j}{\\partial w_{ij}} =\\frac{\\partial }{\\partial w_{ij}} \\sum_k w_{kj} o_k =\\frac{\\partial }{\\partial w_{ij}} w_{ij} o_i =o_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정리하면\n",
    "$$ \\frac{\\partial E}{\\partial w_{ij}}=\\frac{\\partial E}{\\partial o_{j}} \\frac{\\partial o_j}{\\partial net_{j}} \\frac{\\partial net_j}{\\partial w_{ij}}=(o_j - y_j) f(net_j) [1-f(net_j)] o_i.$$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 더 자세한 backpropagation의 설명은 다음을 참조하자.\n",
    "https://en.wikipedia.org/wiki/Backpropagation\n",
    "\n",
    "* keras에서 이용하는 optimizers는 다음을 참조하자.\n",
    "https://keras.io/api/optimizers/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
