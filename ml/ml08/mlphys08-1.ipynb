{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "파라미터 값들을 조정해서 오차를 줄이는 학습과정을 생각해보자. 간단한 신경망에서는 파라미터를 조금씩 바꾸면서 정방향(forward)로 오차의 변화를 계산하고 파라미터 값을 조정하면 된다. 하지만 딥러닝에서는 매우 많은 파라미터가 있어서 이 과정에서 매우 많은 계산이 필요하다. 오차역전파(Error backpropagation)은 이런 상황에서 오차를 최소화하는 파라미터를 찾는 효과적인 방법이다. 이런 이유로 Backpropagation은 딥러닝에서 핵심적인 역할을 한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음과 같은 하나의 히든레이어를 가진 간단한 인공신경망을 생각해보자. 즉, 입력층 - 은닉층 - 출력층으로 구성되어 있으며 입력층과 히든레이어는 복수의 노드로 출력층은 하나의 노드로 이루어져 있다. \n",
    "\n",
    "아래 유도 과정에서 많은 변수가 등장하므로 다음처럼 정리하고 진행하도록 하자. 유도과정 중에 변수가 혼동된다면 언제든지 다시 확인하자.\n",
    "\n",
    "\n",
    "* $i, j$ : node index이며 아래첨자로 표시한다.\n",
    "* $k$ : sample index로 위첨자로 표시한다.\n",
    "* $f(x)$ : activation function (히든레이어, 출력층에서 모두 sigmoid함수를 사용한다고 가정한다.)\n",
    "* $E$ : 오차함수 (mean squared error를 이용한다.)\n",
    "* $t$ : target값\n",
    "* $x_i$ : 입력층 i번째 노드의 값\n",
    "* $h_i$ : 은닉층 i번째 노드의 출력값\n",
    "* $y$ : 출력값 (출력층에서 최종 출력값을 의미한다.)\n",
    "* $w_{ij}$ : 입력층과 은닉층 사이의 weights. 즉 입력층 노드 $i$에서 은닉층 $j$로 향하는 가중치\n",
    "* $\\Omega_i $ : 은닉층과 출력층 사이에 weights. 즉 은닉층 $i$에서 출력층으로 향하는 가중치. 출력층에 하나의 노드만 있다고 가정하였으므로 출력층 노드에 대한 인덱스 생략\n",
    "* $nh_i$ : 은닉층 노드 $i$에 입력값 합. 즉 $nh_j = \\sum_i w_{ij} x_i$. \n",
    "* $ny$ : 출력층 노드 입력값. 즉 $ny = \\sum_i \\Omega_i h_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저 입력값을 넣고 순방향으로 계산하여 오차를 구한다.\n",
    "특정 샘플 하나에 대한 오차(MSE: mean squared error)는 다음과 같다.\n",
    "$$ E = \\frac{1}{2}  (t - y)^2.$$\n",
    "먼저 은닉층과 출력층 사이에 가중치를 살펴보자. 우리가 찾아야하는 것은 파라미터를 바꾸었을 때 에너지 변화이다.\n",
    "$$ \\frac{\\partial E}{\\partial \\Omega_{i}}.$$ \n",
    "간단한 체인룰을 써서 다음처럼 기술해보자.\n",
    "$$ \\frac{\\partial E}{\\partial \\Omega_{i}}=\\frac{\\partial E}{\\partial y} \\frac{\\partial y}{\\partial ny}\n",
    "\\frac{\\partial ny}{\\partial \\Omega_{i}}.$$ \n",
    "여기에서 $ny$는 출력노드에 들어오는 입력의 합을, $y$는 출력값을 뜻한다. 즉 $ny= \\sum_i \\Omega_i h_i$를 뜻한다. \n",
    " \n",
    "위의 변화율 식에서 3개의 항들을 각각 살펴보자.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. $\\frac{\\partial E}{\\partial y}$: 노드의 출력값 변화에 따른 오차의 변화율\n",
    "$$\\frac{\\partial E}{\\partial y} = \\frac{\\partial}{\\partial y} \\frac{1}{2} (t-y)^2 = t -y.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. $\\frac{\\partial y}{\\partial ny}$: 입력합의 변화에 따른 노드의 출력값 변화율\n",
    "$$\\frac{\\partial y}{\\partial ny} = \\frac{\\partial f(ny)}{\\partial ny}=f(ny) [1- f(ny)].$$\n",
    "마지막은 activation 함수를 sigmoid 함수임을 가정하고 유도하였다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. $\\frac{\\partial ny}{\\partial \\Omega_{i}}$: 가중치의 변화에 따른 입력합의 변화율 \n",
    "$$\\frac{\\partial ny}{\\partial \\Omega_{i}} =\\frac{\\partial }{\\partial \\Omega_{i}} \\sum_j \\Omega_j h_j   =h_i.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정리하면\n",
    "$$ \\frac{\\partial E}{\\partial \\Omega_{i}}=\\frac{\\partial E}{\\partial y} \\frac{\\partial y}{\\partial ny}\\frac{\\partial ny}{\\partial \\Omega_{i}} \n",
    "=(t - y) f(ny) [1-f(ny)] h_i.$$ \n",
    "    \n",
    "따라서 다음처럼 정의하면 \n",
    "$$ \\delta_y = (t - y) f'(ny) = (t-y)f(ny) [1-f(ny)]$$\n",
    "$\\Omega_i$를 다음처럼 수정하면 된다. \n",
    "$$ \\Omega_i \\rightarrow \\Omega_i - \\eta \\delta_y h_i.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "같은 방식으로 은닉층의 가중치에 대해 살펴보자.\n",
    "$$ \\frac{\\partial E}{\\partial w_{ij}}=\\frac{\\partial E}{\\partial y} \\frac{\\partial y}{\\partial ny}\\frac{\\partial ny}{\\partial h_i} \\frac{\\partial h_i}{\\partial nh_i} \\frac{\\partial nh_i}{\\partial w_{ij}}.$$ \n",
    "\n",
    "위의 유도과정과 비슷하게 정리하면\n",
    "$$\\delta_{h_i} = \\delta_y \\Omega_i f'(nh_i)$$\n",
    "이며 $w_{ij}$를 다음처럼 수정하면 된다.\n",
    "$$w_{ij}  \\rightarrow w_{ij} -\\eta \\delta_{h_j} x_i $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 더 자세한 backpropagation의 설명은 다음을 참조하자.\n",
    "https://en.wikipedia.org/wiki/Backpropagation\n",
    "\n",
    "* keras에서 이용하는 optimizers는 다음을 참조하자.\n",
    "https://keras.io/api/optimizers/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
